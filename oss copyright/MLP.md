## 📄 MLP

![다층 퍼셉트론](https://user-images.githubusercontent.com/105197533/202836884-e14a78df-8f35-42e7-bc45-dd1028413d6e.png)

> 만들어진 이유

* 퍼셉트론에서 결과값을 내놓는 부분은 결곽 활성 함수인데, 단층 퍼셉트론에서는 이 활성함수가 1개 밖에 없는구조

* 그래서 그 한계로 and연산에 대해서 학습은 가능하지만 XOR에 대해서는 학습이 불가능했음

* 이를 극복하기위해 입력층과 출력층 사이에 하나 이상의 중간층을 두어 비선형적으로 분리되는 데이터 다층 퍼셉트론이 나옴

> 다층 퍼셉트론

* 단층 퍼셉트론은 입력층과 출력층만 존재하지만, 다층 퍼셉트론은 중간에 은닉층(hidden layer)이라는 층을 더 추가

![단층 퍼셉트론](https://user-images.githubusercontent.com/105197533/202836948-977104df-e2c4-4214-8ee2-38174ebe6917.png)

      단층퍼셉트론(은닉층 추가시 다층)

* 이걸 개발함으로써 XOR게이트는 물론 선형 분류만으로 풀지못했던 문제를 풀 수 있게되었음

* 은닉층이 1개 이상이면 다층 퍼셉트론(MLP), 은닉층이 2개 이상인 신경망을 심층 신경망 (Deep Neural Network, DNN)이라 함

* 심층 신경망을 같은 복잡한 문제를 해결하기 위해서는 신경망 층수를 여러층 쌓은 모델을 이용,
깊은 층수를 쌓을 경우 역전파(Backpropagtion) 학습과정에서 데이터가 사라져 학습이 잘 되지 않는 현상인 
Vanishing Gradient 문제가 있었음

* 또한 학습한 내용을 잘 처리하나 새로운 사실을 추론하는 것, 새로운데이터를
처리하는 것을 잘하지 못하는 한계도 있었음

* 이러한 신경망을 '__인공신경망__' 이라고 부름

> 딥러닝

* 이러한 한계를 극복한 인공신경망을 딥러닝(Deep Learning)이라고 함

* 캐나다 토론토 대한 제프리 힌튼 교수는 2006년에 깊은 층수의 신경망 학습시 사전 학습을 통해서 학습함으로써
Vanishing Gradient 문제를 해결할 수 있었음

* 새로운 데이터를 잘 처리하지 못하는 문제는 학습 도중에 고의로 데이터를 누락시키는 방법을 이용하여 해결할 수 있음을 2012년에 밝힘



